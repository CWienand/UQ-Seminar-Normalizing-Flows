\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{multicol}
\geometry{margin=2.4cm}
\usepackage{enumitem}
\usepackage[backend=biber,
            style=authoryear,
            natbib=true,
            maxcitenames=2,
            maxbibnames=10,
            uniquename=false,
            uniquelist=false]{biblatex}



\usepackage{xcolor}

\addbibresource{bibliography.bib}

\title{State of Project}

\author{Seminar on Mathematics for Uncertainty Quantification\\[0.3em]
RWTH Aachen University}
\date{Febuary 10, 2026}

\begin{document}
\maketitle

\section*{Implementation}

Implementation of two distributions (one base, one target) is complete. Base Implementation:
\begin{lstlisting}[language=Python]
class Multivariate_Diag_t(nf.distributions.base.BaseDistribution):


    def __init__(self, loc, diag, dfs):  
       
        super().__init__()
        self.loc = loc
        self.diag = diag
        self.df = dfs
        print([(lo, dia,df)for lo, dia, df in zip(loc,diag,dfs)])
        self.distributions = [torch.distributions.studentT.StudentT(df,lo, dia)for lo, dia,df in zip(loc,diag,dfs)]
        self.n_dims = len(diag)
        self.max_log_prob = 0.0

    def log_prob(self, z):
        return sum([distribution.log_prob(z[:,i]) for i,distribution in enumerate(self.distributions)])
    
    def sample(self, num_samples = 1):
        if type(num_samples) is torch.Size:
            return torch.stack([distribution.sample(num_samples) for distribution in self.distributions],axis = 1)
        else:
            return torch.stack([distribution.sample(torch.Size([num_samples])) for distribution in self.distributions],axis = 1)
    
    def forward(self, num_samples = 1, context = None):
        z = self.sample(num_samples)
        return torch.tensor(z), self.log_prob(z)
\end{lstlisting}

Target Implementation:
\begin{lstlisting}[language=Python]
class Multivariate_t(nf.distributions.target.Target):


    def __init__(self, loc, matrix, df):
       
        super().__init__()
        self.loc = loc
        self.matrix = matrix
        self.df = df
        self.distribution = stats.multivariate_t(loc, matrix,df)
        self.n_dims = len(matrix)
        self.max_log_prob = 0.0

    def log_prob(self, z):
        res = torch.tensor(self.distribution.logpdf(z.detach().numpy()))
        return res
\end{lstlisting}


These implement multivariate t distributions. The constraint of the base to enable sampling is, that the distribution is diagonal. 
The base distribution is implemented in torch, while the target utilizes scipy. 
A type conversion in the training process is now neccessary, which increases training time.

\newpage

\section*{Flows and Hyperparameters}
The flow definition is heavily inspired from the tutorial of backward KLD. I decreased the number of layers to iterate the different Optimizers faster.
In the beginning I encountered random drift, which was due to the target being larger than the base and the Momentum based ADAM optimizer just remembering the 
direction it picked first. I solved this by using SGD.  

\begin{lstlisting}[language=Python]
K = 16
torch.manual_seed(2026)

latent_size = 2
b = torch.Tensor([1 if i \% 2 == 0 else 0 for i in range(latent_size)])
flows = []
for i in range(K):
    s = nf.nets.MLP([latent_size, 2 * latent_size, latent_size], init_zeros=True)
    t = nf.nets.MLP([latent_size, 2 * latent_size, latent_size], init_zeros=True)
    if i \% 2 == 0:
        flows += [nf.flows.MaskedAffineFlow(b, t, s)]
    else:
        flows += [nf.flows.MaskedAffineFlow(1 - b, t, s)]
    flows += [nf.flows.ActNorm(latent_size)]
\end{lstlisting}
\newpage
The training loop is as follows with the following optimizer:

\begin{lstlisting}[language=Python]
max_iter = 50000
num_samples = 2 * 4
anneal_iter = 10000
annealing = False
show_iter = 1000


loss_hist = np.array([])

optimizer = torch.optim.SGD(nfm.parameters(), lr=1e-6, weight_decay=1e-8)
for it in tqdm(range(max_iter)):
    optimizer.zero_grad()
    if annealing:
        loss = nfm.reverse_kld(num_samples, beta=np.min([1., 0.001 + it / anneal_iter]))
    else:
        loss = nfm.reverse_alpha_div(num_samples, dreg=True, alpha=1)
    
    if ~(torch.isnan(loss) | torch.isinf(loss)):
        loss.backward()
        optimizer.step()
    else:
        optimizer = torch.optim.SGD(nfm.parameters(), lr=1e-6)
        print("Loss untypical")
    
    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())
    
    # Plot learned posterior
    if (it + 1) % show_iter == 0:
        log_prob = nfm.log_prob(zz).to('cpu').view(*xx.shape)
        prob = torch.exp(log_prob)
        prob[torch.isnan(prob)] = 0

        plt.figure(figsize=(15, 15))
        plt.pcolormesh(xx, yy, prob.data.numpy())
        plt.contour(xx, yy, prob_target.data.numpy(), cmap=plt.get_cmap('cool'), linewidths=2)
        plt.gca().set_aspect('equal', 'box')
        plt.savefig(f".//Visualization//training_{it}.png",dpi = 600)
        plt.close()
\end{lstlisting}


\section*{Results}
\subsection*{backward-KLD, 2D}
Base and Target distributions:
\begin{lstlisting}
target = cd.Multivariate_t([0,0],[[1,2],[1,2]],20)  
#Loaction, Distribution Scale Matrix, Degrees of freedom
q0 = cd.Multivariate_Diag_t([0,0],[1,1],[10,10])    
#Loaction, Distribution Scale diagonal, Degrees of freedom
\end{lstlisting}
\newpage
\begin{figure}
  \begin{multicols}{2}
  \includegraphics[scale=0.2]{Visualization/init_distributions.png}
  \includegraphics[scale=0.2]{Visualization/training_15999.png}
  \end{multicols}
  \caption{Initial tested distribution as colormap, Target as Elevation lines}
  \label{fig:Init_1}
\end{figure}
\begin{figure}
  \includegraphics[scale=0.2]{Visualization/end_result.png}
  \caption{IniMiddle of Training}
  \label{fig:Result_1}
\end{figure}
\begin{figure}
  \includegraphics[scale=0.7]{Visualization/loss.png}
  \caption{End result}
  \label{fig:Loss_1}
\end{figure}

\end{document}